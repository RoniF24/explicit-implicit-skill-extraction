# ğŸ§  ×”×¡×‘×¨ ××§×™×£ ×¢×œ ×”××•×“×œ×™× ×©×œ SkillSight

## ğŸ“‹ ×ª×•×›×Ÿ ×¢× ×™×™× ×™×
1. [×¡×§×™×¨×” ×›×œ×œ×™×ª](#×¡×§×™×¨×”-×›×œ×œ×™×ª)
2. [××”×™ ×’×™×©×ª Pairwise?](#××”×™-×’×™×©×ª-pairwise)
3. [DeBERTa v3 Base](#1-deberta-v3-base---×”××•×“×œ-×”××•××œ×¥)
4. [RoBERTa Base](#2-roberta-base---××•×“×œ-×—×œ×•×¤×™)
5. [MODELV2 One-Pass](#3-modelv2-one-pass---××•×“×œ-× ×™×¡×™×•× ×™)
6. [×œ××” Pairwise ×¢×“×™×£ ×¢×œ One-Pass?](#×œ××”-pairwise-×¢×“×™×£-×¢×œ-one-pass)
7. [×œ××” ×”××©×§×œ×•×ª ×›×‘×“×™×?](#×œ××”-×§×‘×¦×™-×”××©×§×œ×•×ª-×›×‘×“×™×-×›×œ-×›×š)
8. [×”×©×•×•××” ××¡×›××ª](#×”×©×•×•××”-××¡×›××ª)

---

## ğŸ¯ ×¡×§×™×¨×” ×›×œ×œ×™×ª

×”×¤×¨×•×™×§×˜ ××©×ª××© ×‘-**3 ××•×“×œ×™×** ×œ×–×™×”×•×™ ××™×•×× ×•×™×•×ª ××˜×§×¡×˜×™× ×©×œ ×§×•×¨×•×ª ×—×™×™×:

| ××•×“×œ | ×’×•×“×œ ×§×•×‘×¥ | Macro F1 | ×¡×•×’ ×’×™×©×” |
|------|-----------|----------|----------|
| **DeBERTa v3 Base** | ~704 MB | **97.07%** | Pairwise |
| **RoBERTa Base** | ~476 MB | **95.46%** | Pairwise |
| **MODELV2 One-Pass** | ~704 MB | **48.30%** | One-Pass |

---

## ğŸ”„ ××”×™ ×’×™×©×ª Pairwise?

### ×”×¨×¢×™×•×Ÿ ×”××¨×›×–×™
×‘××§×•× ×œ× ×¡×•×ª ×œ×–×”×•×ª ××ª ×›×œ 136 ×”××™×•×× ×•×™×•×ª ×‘×”×¨×¦×” ××—×ª, ×× ×—× ×• ×©×•××œ×™× ××ª ×”××•×“×œ ×©××œ×” ×¤×©×•×˜×” ×™×•×ª×¨:
> "×”×× ×”×˜×§×¡×˜ ×”×–×” ××›×™×œ ××ª ×”××™×•×× ×•×ª ×”×–×•?"

### ××™×š ×–×” ×¢×•×‘×“ - ×“×•×’××” ××œ××”

× × ×™×— ×©×™×© ×œ× ×• ××ª ×”×˜×§×¡×˜:
```
"I have 5 years of experience with Python and Django. 
Built REST APIs and deployed using Docker."
```

**×©×œ×‘ 1: ×™×•×¦×¨×™× ×–×•×’×•×ª (Pairs)**

×”××•×“×œ ××§×‘×œ ×–×•×’×•×ª ×©×œ `[××™×•×× ×•×ª, ×˜×§×¡×˜]`:

```
Pair 1: ["Python", "I have 5 years of experience..."]   â†’ ?
Pair 2: ["Django", "I have 5 years of experience..."]   â†’ ?
Pair 3: ["Docker", "I have 5 years of experience..."]   â†’ ?
Pair 4: ["AWS", "I have 5 years of experience..."]      â†’ ?
...
Pair 136: ["GraphQL", "I have 5 years..."]              â†’ ?
```

**×©×œ×‘ 2: ×œ×›×œ ×–×•×’ - ×¡×™×•×•×’ ×œ-3 ×§×˜×’×•×¨×™×•×ª**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Output Classes                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Label 0 (NONE)     = ××™×Ÿ ×§×©×¨ ×‘×™×Ÿ ×”×˜×§×¡×˜ ×œ××™×•×× ×•×ª          â”‚
â”‚  Label 1 (IMPLICIT) = ×”××™×•×× ×•×ª × ×¨××–×ª/××©×ª××¢×ª ××”×”×§×©×¨        â”‚
â”‚  Label 2 (EXPLICIT) = ×”××™×•×× ×•×ª ××•×–×›×¨×ª ×‘××¤×•×¨×© ×‘×˜×§×¡×˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**×©×œ×‘ 3: ×ª×•×¦××•×ª ×”×“×•×’××”**
```
["Python", text]  â†’ 2 (EXPLICIT) âœ… ×›×™ "Python" ××•×–×›×¨ ×‘××¤×•×¨×©
["Django", text]  â†’ 2 (EXPLICIT) âœ… ×›×™ "Django" ××•×–×›×¨ ×‘××¤×•×¨×©
["Docker", text]  â†’ 2 (EXPLICIT) âœ… ×›×™ "Docker" ××•×–×›×¨ ×‘××¤×•×¨×©
["REST API", text]â†’ 2 (EXPLICIT) âœ… ×›×™ "REST APIs" ××•×–×›×¨
["AWS", text]     â†’ 0 (NONE)     âŒ ×œ× ××•×–×›×¨ ×‘×˜×§×¡×˜
["DevOps", text]  â†’ 1 (IMPLICIT) ğŸ”¶ ××©×ª××¢ ×-Docker deployment
```

### ×”×§×•×“ ×‘×¤×•×¢×œ - ×›×™×¦×“ ×™×™×©×× ×•

**×§×•×“ ×”×–× ×ª ×–×•×’ ×œ××•×“×œ** (××ª×•×š `analyze_resume.py`):
```python
def predict_pairwise(model_dir: Path, text: str, skills: list):
    tokenizer = AutoTokenizer.from_pretrained(str(model_dir))
    model = AutoModelForSequenceClassification.from_pretrained(str(model_dir))
    
    id_to_score = {0: 0.0, 1: 0.5, 2: 1.0}  # NONE=0, IMPLICIT=0.5, EXPLICIT=1.0
    results = {}
    
    for skill in skills:  # ×¢×•×‘×¨×™× ×¢×œ ×›×œ 136 ×”××™×•×× ×•×™×•×ª
        # ×”×˜×•×§× ×™×™×–×¨ ××§×‘×œ ×–×•×’: (skill, text)
        enc = tokenizer(skill, text, truncation=True, max_length=384, 
                       padding=True, return_tensors="pt")
        
        out = model(**enc)  # forward pass
        pred_id = torch.argmax(out.logits, dim=-1).item()  # 0, 1, ××• 2
        results[skill] = id_to_score[pred_id]
    
    return results
```

### ××‘× ×” ×”×§×œ×˜ ×”×¤× ×™××™ ×œ××•×“×œ

×›×©×× ×—× ×• ×§×•×¨××™× `tokenizer(skill, text)`, × ×•×¦×¨ ×”×§×œ×˜ ×”×‘×:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input Format for Transformer                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  [CLS] Python [SEP] I have 5 years of experience with ... [SEP]    â”‚
â”‚    â†‘      â†‘     â†‘                      â†‘                     â†‘     â”‚
â”‚  Token  Skill  Sep                   Text                  End     â”‚
â”‚  ××™×•×—×“  ×©×     ×”×¤×¨×“×”                 ×ª×•×›×Ÿ ×”×§×•×¨×•×ª ×—×™×™×      ×¡×™×•×   â”‚
â”‚                                                                     â”‚
â”‚  Token IDs:  [101, 18750, 102, 1045, 1138, 1019, 2086, ...]        â”‚
â”‚  Attention:  [1,   1,     1,   1,    1,    1,    1,    ...]        â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1. DeBERTa v3 Base - ×”××•×“×œ ×”××•××œ×¥ âœ…

### ğŸ”¬ ××” ×–×” DeBERTa?
**DeBERTa** (Decoding-enhanced BERT with disentangled attention) ×”×•× ××•×“×œ ×©×¤×” ××ª×§×“× ×©×¤×•×ª×— ×¢×œ ×™×“×™ **Microsoft** ×‘-2020.

### ğŸ§¬ ××™×š DeBERTa ×¢×•×‘×“ - ×”×¡×‘×¨ ×”××œ×’×•×¨×™×ª×

#### ××œ×’×•×¨×™×ª× 1: Disentangled Attention (×ª×©×•××ª ×œ×‘ ××•×¤×¨×“×ª)

×”×¨×¢×™×•×Ÿ ×”××”×¤×›× ×™ ×©×œ DeBERTa: **×œ×”×¤×¨×™×“ ××ª ×”×ª×•×›×Ÿ ××”××™×§×•×**.

×‘-BERT ×¨×’×™×œ, ×›×œ ××™×œ×” ××™×•×¦×’×ª ×›:
```
embedding = content + position
```

×‘-DeBERTa, ××¤×¨×™×“×™× ××•×ª×:
```
content_embedding = ××” ×”××™×œ×” ××•××¨×ª
position_embedding = ××™×¤×” ×”××™×œ×” × ××¦××ª
```

**×œ××” ×–×” ×¢×•×–×¨?** ×“×•×’××”:

```
××©×¤×˜ 1: "Python experience required"
××©×¤×˜ 2: "Required Python experience"
```

×©× ×™ ×”××©×¤×˜×™× ××•××¨×™× ××•×ª×• ×“×‘×¨, ××‘×œ ×”××™×§×•××™× ×©×•× ×™×!
DeBERTa ××‘×™×Ÿ ×©×”×ª×•×›×Ÿ ×–×”×” ×œ××¨×•×ª ×”××™×§×•× ×”×©×•× ×”.

#### ××™×š ×—×™×©×•×‘ ×”-Attention ×¢×•×‘×“ ×‘-DeBERTa

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               DeBERTa Disentangled Attention                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Standard BERT Attention:                                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       â”‚
â”‚  A = softmax(Q Ã— K^T / âˆšd)                                     â”‚
â”‚  Q, K ××›×™×œ×™× ×ª×•×›×Ÿ+××™×§×•× ××¢×•×¨×‘×‘×™×                                â”‚
â”‚                                                                 â”‚
â”‚  DeBERTa Attention (4 ×¨×›×™×‘×™× × ×¤×¨×“×™×):                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                           â”‚
â”‚  A = softmax((Q_c Ã— K_c^T)   +    # ×ª×•×›×Ÿ-×œ×ª×•×›×Ÿ                 â”‚
â”‚              (Q_c Ã— K_p^T)   +    # ×ª×•×›×Ÿ-×œ××™×§×•×                â”‚
â”‚              (Q_p Ã— K_c^T)   +    # ××™×§×•×-×œ×ª×•×›×Ÿ                â”‚
â”‚              (Q_p Ã— K_p^T))      # ××™×§×•×-×œ××™×§×•×                â”‚
â”‚                                                                 â”‚
â”‚  Q_c = Query ×©×œ ×ª×•×›×Ÿ     K_c = Key ×©×œ ×ª×•×›×Ÿ                     â”‚
â”‚  Q_p = Query ×©×œ ××™×§×•×    K_p = Key ×©×œ ××™×§×•×                    â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### ××œ×’×•×¨×™×ª× 2: Enhanced Mask Decoder (EMD)

×‘×–××Ÿ ××™××•×Ÿ, DeBERTa ××•×¡×™×£ ××™×“×¢ ×¢×œ ×”××™×§×•× ×”××•×—×œ×˜ ×¨×§ ×‘×©×›×‘×” ×”××—×¨×•× ×”:

```python
# Pseudocode ×©×œ EMD
def enhanced_mask_decoder(hidden_states, absolute_positions):
    # 11 ×©×›×‘×•×ª ×¨××©×•× ×•×ª - ×¨×§ relative position
    for layer in layers[:-1]:
        hidden = layer(hidden, relative_position_only=True)
    
    # ×©×›×‘×” ××—×¨×•× ×” - ××•×¡×™×¤×™× absolute position
    final_layer = layers[-1]
    output = final_layer(hidden, absolute_position=absolute_positions)
    
    return output
```

**×œ××” ×–×” ×¢×•×–×¨?** ×”××•×“×œ ×œ×•××“ ×“×¤×•×¡×™× ×›×œ×œ×™×™× ×™×•×ª×¨ ×‘×©×›×‘×•×ª ×”×¨××©×•× ×•×ª.

### ğŸ“Š ××¤×¨×˜ ×˜×›× ×™

| ×¤×¨××˜×¨ | ×¢×¨×š | ×”×¡×‘×¨ |
|-------|-----|------|
| ××¡×¤×¨ ×©×›×‘×•×ª (Layers) | 12 | 12 Transformer blocks |
| Attention Heads | 12 | ××¡×ª×›×œ×™× ×¢×œ ×”×˜×§×¡×˜ ×-12 ×–×•×•×™×•×ª |
| Hidden Size | 768 | ×’×•×“×œ ×”×™×™×¦×•×’ ×”×¤× ×™××™ ×©×œ ×›×œ ×˜×•×§×Ÿ |
| Vocabulary Size | 128,100 | ×›××•×ª ×”××™×œ×™×/×—×œ×§×™ ××™×œ×™× ×”××•×›×¨×™× |
| Max Sequence Length | 512 | ××•×¨×š ××§×¡×™××œ×™ ×©×œ ×§×œ×˜ |
| ××¡×¤×¨ ×¤×¨××˜×¨×™× | ~184 ××™×œ×™×•×Ÿ | ×›×œ ×”××©×§×œ×•×ª ×”× ×™×ª× ×•×ª ×œ×œ××™×“×” |
| ×’×•×“×œ ×§×•×‘×¥ | **~704 MB** | 184M Ã— 4 bytes â‰ˆ 736MB |

### ğŸ“ ××™×š ××™×× ×• ××ª DeBERTa

**×§×•×‘×¥ ×”××™××•×Ÿ: `src/training/model_creation.py`**

```python
# ×©×œ×‘ 1: ×˜×¢×™× ×ª ×”××•×“×œ ×”×‘×¡×™×¡×™ ×-HuggingFace
model = AutoModelForSequenceClassification.from_pretrained(
    "microsoft/deberta-v3-base", 
    num_labels=3  # NONE, IMPLICIT, EXPLICIT
)

# ×©×œ×‘ 2: ×™×¦×™×¨×ª ×–×•×’×•×ª ×œ××™××•×Ÿ
def build_pairs(examples, global_skills, neg_ratio=4.0):
    rows = []
    for ex in examples:
        text = ex["job_description"]
        skills_dict = ex["skills"]  # {"Python": 1.0, "AWS": 0.5, ...}
        
        # ×–×•×’×•×ª ×—×™×•×‘×™×™× (××™×•×× ×•×™×•×ª ×©×§×™×™××•×ª)
        for skill, score in skills_dict.items():
            label = score_to_label(score)  # 0, 1, ××• 2
            if label > 0:
                rows.append({
                    "skill": skill, 
                    "job_description": text, 
                    "label": label
                })
        
        # ×–×•×’×•×ª ×©×œ×™×œ×™×™× (××™×•×× ×•×™×•×ª ×©×œ× ×§×™×™××•×ª)
        negatives = sample_negatives(global_skills, skills_dict, ratio=4)
        for skill in negatives:
            rows.append({
                "skill": skill, 
                "job_description": text, 
                "label": 0
            })
    
    return rows
```

**×©×œ×‘ 3: ×˜×•×§× ×™×–×¦×™×”**
```python
def tokenize_pairs(tokenizer, rows, max_length=384):
    skills = [r["skill"] for r in rows]      # ["Python", "AWS", ...]
    texts = [r["job_description"] for r in rows]
    labels = [r["label"] for r in rows]
    
    # ×™×•×¦×¨ ××ª ×”×§×œ×˜ ×‘×¤×•×¨××˜ [CLS] skill [SEP] text [SEP]
    encodings = tokenizer(skills, texts, 
                          truncation=True, 
                          max_length=max_length)
    
    return PairDataset(encodings, labels)
```

**×©×œ×‘ 4: ××™××•×Ÿ ×¢× HuggingFace Trainer**
```python
training_args = TrainingArguments(
    output_dir="models/deberta_v3_base",
    num_train_epochs=3,
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="macro_f1",
    fp16=True,  # ×—×¦×™ ×“×™×•×§ ×œ×—×™×¡×›×•×Ÿ ×‘×–×™×›×¨×•×Ÿ
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=val_ds,
    compute_metrics=compute_metrics,  # ××—×©×‘ F1, Precision, Recall
)

trainer.train()  # ××ª×—×™×œ ××™××•×Ÿ
trainer.save_model("models/deberta_v3_base")  # ×©×•××¨ ××ª ×”××©×§×œ×•×ª
```

### ğŸ¯ ×ª×•×¦××•×ª ×¢×œ ×”× ×ª×•× ×™× ×©×œ× ×•

```
              precision    recall  f1-score   support
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
No Match (0)    0.9902    0.9802    0.9852     9,556
Implicit (1)    0.8997    0.9477    0.9231     1,779
Explicit (2)    0.9951    0.9967    0.9959       610
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Accuracy                            0.9762    11,945
Macro F1                            0.9681
Best Validation F1                  0.9707
```

### ğŸ’¡ ×œ××” DeBERTa ×”×›×™ ×˜×•×‘?

1. **Disentangled Attention**: ××‘×™×Ÿ ×”×§×©×¨×™× ×’× ×›×©×”××™×§×•× ××©×ª× ×”
2. **Vocabulary ×’×“×•×œ (128K)**: ××›×™×¨ ×™×•×ª×¨ ××•× ×—×™× ×˜×›× ×™×™×
3. **Enhanced Mask Decoder**: ×œ××™×“×” ×™×¦×™×‘×” ×™×•×ª×¨
4. **Relative Position Encoding**: ××‘×™×Ÿ ×™×—×¡×™× ×‘×™×Ÿ ××™×œ×™× ×¨×—×•×§×•×ª

---

## 2. RoBERTa Base - ××•×“×œ ×—×œ×•×¤×™

### ğŸ”¬ ××” ×–×” RoBERTa?
**RoBERTa** (Robustly Optimized BERT) ×”×•× ××•×“×œ ×©×¤×•×ª×— ×¢×œ ×™×“×™ **Facebook AI** ×‘-2019. ×”×•× ×’×¨×¡×” ××©×•×¤×¨×ª ×©×œ BERT ×¢× ××™××•×Ÿ ××•×¤×˜×™××œ×™ ×™×•×ª×¨.

### ğŸ§¬ ××™×š RoBERTa ×¢×•×‘×“ - ×”×¡×‘×¨ ×”××œ×’×•×¨×™×ª×

RoBERTa ×”×•× ×‘×¢×¦× BERT ×¢× ×©×™× ×•×™×™× ×‘××•×¤×Ÿ ×”××™××•×Ÿ:

#### ××œ×’×•×¨×™×ª× 1: Dynamic Masking (××™×¡×•×š ×“×™× ××™)

**×‘-BERT ×”××§×•×¨×™:**
```python
# ×”× ×ª×•× ×™× × ×•×¦×¨×• ×¤×¢× ××—×ª ×¢× ××¡×›×•×ª ×§×‘×•×¢×•×ª
masked_data = create_masked_data(corpus)  # ××¨×™×¦×™× ×¤×¢× ××—×ª
for epoch in range(10):
    train(model, masked_data)  # ××•×ª×Ÿ ××¡×›×•×ª ×‘×›×œ epoch
```

**×‘-RoBERTa:**
```python
# ××¡×›×•×ª ××©×ª× ×•×ª ×‘×›×œ epoch!
for epoch in range(10):
    masked_data = create_masked_data(corpus)  # ×™×•×¦×¨×™× ××¡×›×•×ª ×—×“×©×•×ª
    train(model, masked_data)  # ××¡×›×•×ª ×©×•× ×•×ª ×‘×›×œ ×¤×¢×
```

**×œ××” ×–×” ×¢×•×–×¨?** ×”××•×“×œ ×¨×•××” ××ª ××•×ª×Ÿ ××™×œ×™× ×‘××¦×‘×™× ×©×•× ×™× â†’ ×œ××™×“×” ××’×•×•× ×ª ×™×•×ª×¨.

#### ××œ×’×•×¨×™×ª× 2: ×œ×œ× Next Sentence Prediction (NSP)

**×‘-BERT ×”××§×•×¨×™:**
```python
# ××©×™××” 1: ×—×™×–×•×™ ××™×œ×™× ××•×¡×ª×¨×•×ª
loss1 = masked_language_model(text)

# ××©×™××” 2: ×”×× ××©×¤×˜ B ×‘× ××—×¨×™ ××©×¤×˜ A? (×œ× ×™×¢×™×œ!)
loss2 = next_sentence_prediction(sentence_A, sentence_B)

total_loss = loss1 + loss2
```

**×‘-RoBERTa:**
```python
# ×¨×§ ××©×™××” 1! ×™×•×ª×¨ ×™×¢×™×œ
loss = masked_language_model(text)
```

×”××—×§×¨ ×”×¨××” ×©-NSP ×œ× ×¢×•×–×¨ ×•××¤×™×œ×• ××–×™×§ ×œ×‘×™×¦×•×¢×™×.

### ğŸ“Š ××¤×¨×˜ ×˜×›× ×™

| ×¤×¨××˜×¨ | ×¢×¨×š | ×”×©×•×•××” ×œ-DeBERTa |
|-------|-----|------------------|
| ××¡×¤×¨ ×©×›×‘×•×ª (Layers) | 12 | ×–×”×” |
| Attention Heads | 12 | ×–×”×” |
| Hidden Size | 768 | ×–×”×” |
| Vocabulary Size | 50,265 | **×¤×—×•×ª ×-DeBERTa (128K)** |
| Max Sequence Length | 514 | ×“×•××” |
| ××¡×¤×¨ ×¤×¨××˜×¨×™× | ~125 ××™×œ×™×•×Ÿ | **×¤×—×•×ª ×-DeBERTa (184M)** |
| ×’×•×“×œ ×§×•×‘×¥ | **~476 MB** | **228MB ×¤×—×•×ª!** |

### ğŸ“ ××™×š ××™×× ×• ××ª RoBERTa

××•×ª×• ×ª×”×œ×™×š ×‘×“×™×•×§ ×›××• DeBERTa, ×¨×§ ×¢× ××•×“×œ ×‘×¡×™×¡ ×©×•× ×”:

```python
# ×”×”×‘×“×œ ×”×™×—×™×“ - ×©× ×”××•×“×œ
model = AutoModelForSequenceClassification.from_pretrained(
    "roberta-base",  # ×‘××§×•× "microsoft/deberta-v3-base"
    num_labels=3
)
```

**×¤×§×•×“×ª ×”××™××•×Ÿ:**
```powershell
python src/training/model_creation.py --train \
    --model roberta-base \
    --out models/roberta_base \
    --epochs 3 \
    --lr 2e-5 \
    --batch 8
```

### ğŸ¯ ×ª×•×¦××•×ª

- **Best Validation Macro F1**: 95.46%
- **Best Checkpoint**: checkpoint-20826

### ğŸ’¡ ×™×ª×¨×•× ×•×ª ×•×—×¡×¨×•× ×•×ª

| ×™×ª×¨×•×Ÿ | ×—×¡×¨×•×Ÿ |
|-------|-------|
| ×§×•×‘×¥ ×§×˜×Ÿ ×™×•×ª×¨ (476MB) | F1 × ××•×š ×‘-1.6% ×-DeBERTa |
| ××”×™×¨ ×™×•×ª×¨ ×‘××™××•×Ÿ | Vocabulary ×§×˜×Ÿ ×™×•×ª×¨ (50K vs 128K) |
| ×¤×—×•×ª ×–×™×›×¨×•×Ÿ GPU | ×œ× Disentangled Attention |
| ××™××•×Ÿ ×™×¦×™×‘ | Position Encoding ×¤×—×•×ª ××ª×•×—×›× |

---

## 3. MODELV2 One-Pass - ××•×“×œ × ×™×¡×™×•× ×™ âŒ

### ğŸ”¬ ××” ×–×” One-Pass?
×’×™×©×ª **One-Pass** ×× ×¡×” ×œ×–×”×•×ª ××ª ×›×œ ×”××™×•×× ×•×™×•×ª ×‘×”×¨×¦×” ××—×ª, ×‘××§×•× ×œ×‘×“×•×§ ×›×œ ××™×•×× ×•×ª ×‘× ×¤×¨×“.

### ğŸ§¬ ××™×š One-Pass ×¢×•×‘×“ - ×”×¡×‘×¨ ×”××œ×’×•×¨×™×ª×

#### ××¨×›×™×˜×§×˜×•×¨×ª ×”×¨×©×ª

```python
# ××ª×•×š src/MODELV2/modelv2.py

class OnePassSkillClassifier(nn.Module):
    def __init__(self, base_name: str, num_skills: int, dropout: float = 0.1):
        super().__init__()
        # ×©×›×‘×” 1: ××•×“×œ ×‘×¡×™×¡ (DeBERTa)
        self.base = AutoModel.from_pretrained(base_name)
        hidden = self.base.config.hidden_size  # 768
        
        self.num_skills = num_skills  # 136 ××™×•×× ×•×™×•×ª
        self.dropout = nn.Dropout(dropout)
        
        # ×©×›×‘×” 2: Classification Head
        # ××•×¦×™× 136 Ã— 3 = 408 ×¢×¨×›×™×!
        self.head = nn.Linear(hidden, num_skills * 3)
        
        self.loss_fn = nn.CrossEntropyLoss()
```

#### Forward Pass - ××” ×§×•×¨×” ×‘×”×¨×¦×”

```python
def forward(self, input_ids, attention_mask, labels=None):
    # ×©×œ×‘ 1: ××¢×‘×™×¨×™× ××ª ×”×˜×§×¡×˜ ×“×¨×š Transformer
    out = self.base(input_ids=input_ids, attention_mask=attention_mask)
    
    # ×©×œ×‘ 2: ×œ×•×§×—×™× ×¨×§ ××ª ×”-CLS token
    last_hidden = out.last_hidden_state  # ×¦×•×¨×”: [Batch, Tokens, 768]
    cls = last_hidden[:, 0, :]           # ×¦×•×¨×”: [Batch, 768]
    
    # ×©×œ×‘ 3: Dropout ×œ×× ×™×¢×ª overfitting
    x = self.dropout(cls)
    
    # ×©×œ×‘ 4: Classification Head
    logits = self.head(x)  # ×¦×•×¨×”: [Batch, 136 Ã— 3 = 408]
    
    # ×©×œ×‘ 5: Reshape ×œ×¤×•×¨××˜ [Batch, 136 skills, 3 classes]
    logits = logits.view(-1, self.num_skills, 3)
    
    # ×©×œ×‘ 6: ×—×™×©×•×‘ Loss
    if labels is not None:
        # labels ×¦×•×¨×”: [Batch, 136] - ×¢×¨×›×™× 0,1,2 ×œ×›×œ skill
        B, S = labels.shape
        flat_logits = logits.reshape(B * S, 3)  # [BatchÃ—136, 3]
        flat_labels = labels.reshape(B * S)     # [BatchÃ—136]
        loss = self.loss_fn(flat_logits, flat_labels)
    
    return {"loss": loss, "logits": logits}
```

### ğŸ“ ×”×©×•×•××” ×•×™×–×•××œ×™×ª: Pairwise vs One-Pass

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Pairwise (DeBERTa/RoBERTa)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  Input:                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ [CLS] Python [SEP] I have experience with... [SEP]â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                          â†“                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚         Transformer (12 layers)                   â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                          â†“                                          â”‚
â”‚  Output: [0.02, 0.08, 0.90] â†’ Class 2 (EXPLICIT)                   â”‚
â”‚                                                                     â”‚
â”‚  ××¨×™×¦×™× 136 ×¤×¢××™× (×¤×¢× ×œ×›×œ ××™×•×× ×•×ª!)                                â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      One-Pass (MODELV2)                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  Input:                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ [CLS] I have experience with Python... [SEP]      â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                          â†“                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚         Transformer (12 layers)                   â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                          â†“                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚      Linear Layer: 768 â†’ 408 (136 Ã— 3)           â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                          â†“                                          â”‚
â”‚  Output: [[.9,.05,.05], [.1,.8,.1], ...] (136 predictions!)        â”‚
â”‚           Python=0     Django=1    ...                              â”‚
â”‚                                                                     â”‚
â”‚  ××¨×™×¦×™× ×¤×¢× ××—×ª ×‘×œ×‘×“!                                               â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ“Š ××¤×¨×˜ ×˜×›× ×™

| ×¤×¨××˜×¨ | ×¢×¨×š |
|-------|-----|
| ×‘×¡×™×¡ | microsoft/deberta-v3-base |
| Epochs | 3 |
| Learning Rate | 2e-05 |
| Batch Size | 8 |
| Max Length | 384 |
| Output Size | 136 Ã— 3 = 408 |

### ğŸ¯ ×ª×•×¦××•×ª ×¢×œ ×”× ×ª×•× ×™× ×©×œ× ×•

```
              precision    recall  f1-score   support
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
No Match (0)    0.9703    0.9996    0.9848    68,978
Implicit (1)    0.8485    0.1088    0.1928     1,802
Explicit (2)    0.8962    0.1532    0.2617       620
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Macro F1                            0.4798
Accuracy                            0.9698
```

### âš ï¸ ×œ××” ×”×‘×™×¦×•×¢×™× ×’×¨×•×¢×™×?

#### ×‘×¢×™×” 1: ×—×•×¡×¨ ××™×–×•×Ÿ ×§×™×¦×•× ×™ ×‘× ×ª×•× ×™×

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Class Distribution                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  No Match (0): â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 96.6%      â”‚
â”‚  Implicit (1): â–ˆâ–ˆ 2.5%                                              â”‚
â”‚  Explicit (2): â–ˆ 0.9%                                               â”‚
â”‚                                                                     â”‚
â”‚  ×”××•×“×œ ×œ×•××“ ×¤×©×•×˜ ×œ×”×’×™×“ "0" ×œ×›×œ ×“×‘×¨!                                 â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**×‘× ×ª×•× ×™ ×”××™××•×Ÿ:**
- 68,978 ×“×•×’×××•×ª ×©×œ "No Match" (96.6%)
- 1,802 ×“×•×’×××•×ª ×©×œ "Implicit" (2.5%)
- 620 ×“×•×’×××•×ª ×©×œ "Explicit" (0.9%)

×”××•×“×œ ×œ×•××“ ×œ×—×–×•×ª "0" ×ª××™×“ ×›×™ ×–×” × ×›×•×Ÿ ×‘-97% ××”××§×¨×™×!

#### ×‘×¢×™×” 2: Recall × ××•×š ×××•×“

```
Implicit: Recall = 10.88% â†’ ××–×”×” ×¨×§ 1 ××›×œ 10!
Explicit: Recall = 15.32% â†’ ××–×”×” ×¨×§ 1.5 ××›×œ 10!
```

×”××•×“×œ ××¤×¡×¤×¡ ××ª ×¨×•×‘ ×”××™×•×× ×•×™×•×ª ×”×××™×ª×™×•×ª.

#### ×‘×¢×™×” 3: Multi-Label ×§×©×” ××“×™

×‘-Pairwise: ×”××•×“×œ ×œ×•××“ ×©××œ×” ××—×ª ×¤×©×•×˜×”
```
"×”×× Python ×§×™×™× ×‘×˜×§×¡×˜?" â†’ ×›×Ÿ/×œ×/××©×ª××¢
```

×‘-One-Pass: ×”××•×“×œ ×œ×•××“ 136 ×©××œ×•×ª ×‘×•-×–×× ×™×ª
```
"×”×× Python ×§×™×™×?" + "×”×× AWS ×§×™×™×?" + "×”×× Docker ×§×™×™×?" + ...
```

×–×” ×“×•×¨×© ×”×¨×‘×” ×™×•×ª×¨ × ×ª×•× ×™× ×•×”×¨×‘×” ×™×•×ª×¨ ××™××•×Ÿ.

---

## âš–ï¸ ×œ××” Pairwise ×¢×“×™×£ ×¢×œ One-Pass?

### 1. ×¤×™×©×•×˜ ×”×‘×¢×™×”

| ×’×™×©×” | ××” ×”××•×“×œ ×œ×•××“ | ×§×•×©×™ |
|------|--------------|------|
| Pairwise | "×”×× skill X ×§×™×™× ×‘×˜×§×¡×˜?" | **×§×œ** - ×©××œ×” ×‘×™× ××¨×™×ª |
| One-Pass | "××™×œ×• ×-136 skills ×§×™×™××™×?" | **×§×©×”** - Multi-label |

### 2. ××™×–×•×Ÿ × ×ª×•× ×™×

**Pairwise:**
```python
# ×©×•×œ×˜×™× ×‘×™×—×¡ ×‘×™×Ÿ ×“×•×’×××•×ª ×—×™×•×‘×™×•×ª ×œ×©×œ×™×œ×™×•×ª
neg_ratio = 4.0  # 4 ×©×œ×™×œ×™×•×ª ×œ×›×œ ×—×™×•×‘×™×ª
positives = [("Python", text, 2), ("Django", text, 1)]
negatives = sample(4 * len(positives), from_all_skills)
# ×™×—×¡: 1:4 - ×”×¨×‘×” ×™×•×ª×¨ ×××•×–×Ÿ!
```

**One-Pass:**
```python
# ××™×Ÿ ×©×œ×™×˜×” - ×”×™×—×¡ × ×§×‘×¢ ××”× ×ª×•× ×™×
# ×œ×›×œ ×˜×§×¡×˜ ×™×© ~3-5 ××™×•×× ×•×™×•×ª ××ª×•×š 136
# ×™×—×¡ ×××™×ª×™: 1:40+ - ×œ× ×××•×–×Ÿ ×‘×›×œ×œ!
```

### 3. ×¡×™×’× ×œ ×œ××™×“×”

**Pairwise:**
```
×›×œ forward pass â†’ loss ×¢×œ ×©××œ×” ××—×ª ×¡×¤×¦×™×¤×™×ª
×”×’×¨×“×™×× ×˜: "×ª×œ××“ ×œ×–×”×•×ª Python ×˜×•×‘ ×™×•×ª×¨"
```

**One-Pass:**
```
×›×œ forward pass â†’ loss ×××•×¦×¢ ×¢×œ 136 ×©××œ×•×ª
×”×’×¨×“×™×× ×˜: "×ª×œ××“ ×œ×–×”×•×ª ×”×›×œ ×‘×™× ×•× ×™" ğŸ˜•
```

### 4. ×”×©×•×•××” ××¡×¤×¨×™×ª

| ××“×“ | Pairwise (DeBERTa) | One-Pass |
|-----|-------------------|----------|
| **Macro F1** | 97.07% | 48.30% |
| **Recall (Implicit)** | 94.77% | 10.88% |
| **Recall (Explicit)** | 99.67% | 15.32% |
| ××”×™×¨×•×ª ×”×¨×¦×” | ××™×˜×™ (136 passes) | ××”×™×¨ (1 pass) |
| ××™×›×•×ª | âœ… ××¢×•×œ×” | âŒ ×’×¨×•×¢×” |

### 5. ××ª×™ One-Pass ×™×›×•×œ ×œ×¢×‘×•×“?

One-Pass ×™×›×•×œ ×œ×¢×‘×•×“ ××:
- ×™×© ×”×¨×‘×” ×™×•×ª×¨ × ×ª×•× ×™× (×¤×™ 10-100)
- ××©×ª××©×™× ×‘-Class Weights ××• Focal Loss
- ×¢×•×©×™× Data Augmentation
- ×××× ×™× ×™×•×ª×¨ epochs

**××‘×œ ×œ× ×ª×•× ×™× ×”× ×•×›×—×™×™× ×©×œ× ×• - Pairwise ×¤×©×•×˜ ×¢×“×™×£!**

---

## ğŸ’¾ ×œ××” ×§×‘×¦×™ ×”××©×§×œ×•×ª ×›×‘×“×™× ×›×œ ×›×š?

### ğŸ§® ××” ×™×© ×‘×ª×•×š ×§×•×‘×¥ ×”××©×§×œ×•×ª?

×§×•×‘×¥ `model.safetensors` ××›×™×œ ××ª ×›×œ ×”×¤×¨××˜×¨×™× ×”× ×œ××“×™× ×©×œ ×”×¨×©×ª:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ××‘× ×” model.safetensors                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  1. EMBEDDINGS LAYER (~98-392 MB)                                  â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Token Embeddings:                                           â”‚   â”‚
â”‚  â”‚  â€¢ DeBERTa: 128,100 tokens Ã— 768 dims = 98.4 million params â”‚   â”‚
â”‚  â”‚  â€¢ RoBERTa: 50,265 tokens Ã— 768 dims = 38.6 million params  â”‚   â”‚
â”‚  â”‚                                                              â”‚   â”‚
â”‚  â”‚  Position Embeddings:                                        â”‚   â”‚
â”‚  â”‚  â€¢ DeBERTa: Relative (××•×¨×›×‘ ×™×•×ª×¨, ~100MB × ×•×¡×¤×™×)            â”‚   â”‚
â”‚  â”‚  â€¢ RoBERTa: Absolute (512 Ã— 768 = 393K params)              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                     â”‚
â”‚  2. 12 TRANSFORMER LAYERS (~350-400 MB)                            â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  ×›×œ ×©×›×‘×” ××›×™×œ×”:                                              â”‚   â”‚
â”‚  â”‚                                                              â”‚   â”‚
â”‚  â”‚  Self-Attention:                                             â”‚   â”‚
â”‚  â”‚  â€¢ Q (Query):  768 Ã— 768 = 589,824 params                   â”‚   â”‚
â”‚  â”‚  â€¢ K (Key):    768 Ã— 768 = 589,824 params                   â”‚   â”‚
â”‚  â”‚  â€¢ V (Value):  768 Ã— 768 = 589,824 params                   â”‚   â”‚
â”‚  â”‚  â€¢ Output:     768 Ã— 768 = 589,824 params                   â”‚   â”‚
â”‚  â”‚  ×¡×”"×› Attention: 2.36M params Ã— 12 layers = 28.3M           â”‚   â”‚
â”‚  â”‚                                                              â”‚   â”‚
â”‚  â”‚  Feed-Forward Network:                                       â”‚   â”‚
â”‚  â”‚  â€¢ Linear1: 768 Ã— 3072 = 2.36M params                       â”‚   â”‚
â”‚  â”‚  â€¢ Linear2: 3072 Ã— 768 = 2.36M params                       â”‚   â”‚
â”‚  â”‚  ×¡×”"×› FFN: 4.72M params Ã— 12 layers = 56.6M                 â”‚   â”‚
â”‚  â”‚                                                              â”‚   â”‚
â”‚  â”‚  LayerNorm: ~3K params Ã— 4 Ã— 12 = 144K                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                     â”‚
â”‚  3. CLASSIFICATION HEAD (~6 KB)                                    â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Linear: 768 â†’ 3 = 2,307 params                             â”‚   â”‚
â”‚  â”‚  (×–× ×™×— ×œ×¢×•××ª ×”×©××¨!)                                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”¢ ×—×™×©×•×‘ ××ª××˜×™ ××¤×•×¨×˜

#### DeBERTa v3 Base:

```python
# Token Embeddings
token_embeddings = vocab_size Ã— hidden_size
                 = 128,100 Ã— 768
                 = 98,380,800 params

# Relative Position Embeddings (×™×™×—×•×“×™ ×œ-DeBERTa)
# ××•×¨×›×‘ ×™×•×ª×¨ - ×›×•×œ×œ position buckets ×•××˜×¨×™×¦×•×ª × ×•×¡×¤×•×ª
relative_pos = ~25,000,000 params  # ×”×¢×¨×›×”

# 12 Transformer Layers
attention_per_layer = 4 Ã— (768 Ã— 768) = 2,359,296
ffn_per_layer = 2 Ã— (768 Ã— 3072) = 4,718,592
layernorm_per_layer = 4 Ã— 768 Ã— 2 = 6,144
total_per_layer = 7,084,032
all_layers = 12 Ã— 7,084,032 = 85,008,384 params

# Classification Head
classifier = 768 Ã— 3 + 3 = 2,307 params

# ×¡×”"×›
total = 98.4M + 25M + 85M + 0.002M â‰ˆ 184M params

# ×‘×‘×ª×™× (float32 = 4 bytes)
file_size = 184,000,000 Ã— 4 = 736 MB
# ××—×¨×™ ×“×—×™×¡×”: ~704 MB
```

#### RoBERTa Base:

```python
# Token Embeddings
token_embeddings = 50,265 Ã— 768 = 38,603,520 params

# Absolute Position Embeddings
position_embeddings = 514 Ã— 768 = 394,752 params

# 12 Transformer Layers (××•×ª×• ××‘× ×”)
all_layers = 85,008,384 params

# Classification Head
classifier = 2,307 params

# ×¡×”"×›
total = 38.6M + 0.4M + 85M + 0.002M â‰ˆ 125M params

# ×‘×‘×ª×™×
file_size = 125,000,000 Ã— 4 = 500 MB
# ××—×¨×™ ×“×—×™×¡×”: ~476 MB
```

### ğŸ¤” ×œ××” DeBERTa ×’×“×•×œ ×™×•×ª×¨ ×‘-228MB?

| ×¨×›×™×‘ | DeBERTa | RoBERTa | ×”×¤×¨×© |
|------|---------|---------|------|
| **Vocabulary** | 128,100 | 50,265 | +77,835 tokens |
| Token Embeddings | 98.4 MB | 38.6 MB | **+59.8 MB** |
| Position Type | Relative | Absolute | - |
| Position Size | ~25M params | 0.4M params | **+~100 MB** |
| Disentangled Matrices | ×›×Ÿ | ×œ× | **+~50 MB** |
| **×¡×”"×›** | 704 MB | 476 MB | **+228 MB** |

### ğŸ“¦ ×œ××” float32 ×•×œ× ××©×”×• ×§×˜×Ÿ ×™×•×ª×¨?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Types & Precision                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  float32 (4 bytes) - ××” ×©×× ×—× ×• ××©×ª××©×™×:                            â”‚
â”‚  â€¢ ×“×™×•×§: 7 ×¡×¤×¨×•×ª ×¢×©×¨×•× ×™×•×ª                                          â”‚
â”‚  â€¢ ×˜×•×•×—: Â±3.4 Ã— 10Â³â¸                                               â”‚
â”‚  â€¢ ×™×ª×¨×•×Ÿ: ×™×¦×™×‘×•×ª ××¡×¤×¨×™×ª ××¢×•×œ×”                                      â”‚
â”‚  â€¢ ×—×™×¡×¨×•×Ÿ: ×’×•×“×œ ×§×•×‘×¥                                               â”‚
â”‚                                                                     â”‚
â”‚  float16 (2 bytes) - ××¤×©×¨×™ ×œ×—×™×¡×›×•×Ÿ:                                â”‚
â”‚  â€¢ ×“×™×•×§: 3 ×¡×¤×¨×•×ª ×¢×©×¨×•× ×™×•×ª                                          â”‚
â”‚  â€¢ ×™×ª×¨×•×Ÿ: ×—×¦×™ ××”×’×•×“×œ! (350MB ×‘××§×•× 700MB)                          â”‚
â”‚  â€¢ ×—×™×¡×¨×•×Ÿ: ×¢×œ×•×œ ×œ×’×¨×•× ×œ-overflow/underflow                         â”‚
â”‚                                                                     â”‚
â”‚  int8 (1 byte) - Quantization:                                     â”‚
â”‚  â€¢ ×™×ª×¨×•×Ÿ: ×¨×‘×¢ ××”×’×•×“×œ! (175MB)                                      â”‚
â”‚  â€¢ ×—×™×¡×¨×•×Ÿ: ×™×¨×™×“×” ×‘××™×›×•×ª ×©×œ 1-3%                                    â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ’¡ ×”×× ××¤×©×¨ ×œ×”×§×˜×™×Ÿ?

**×›×Ÿ! ××¤×©×¨ ×œ×¢×©×•×ª Quantization:**

```python
# ×“×•×’××” ×œ×§×•×•× ×˜×™×–×¦×™×” (×œ× ××™×•×©× ×‘×¤×¨×•×™×§×˜)
from transformers import AutoModelForSequenceClassification
import torch

model = AutoModelForSequenceClassification.from_pretrained("models/deberta_v3_base")

# Dynamic Quantization
quantized_model = torch.quantization.quantize_dynamic(
    model, 
    {torch.nn.Linear},  # ×¨×§ ×©×›×‘×•×ª Linear
    dtype=torch.qint8
)

# ×’×•×“×œ ×—×“×©: ~175MB ×‘××§×•× 704MB!
# ××‘×œ: ×™×¨×™×“×” ×©×œ ~1-2% ×‘-F1
```

**×œ××” ×œ× ×¢×©×™× ×• ××ª ×–×”?**
- 704MB ×¢×“×™×™×Ÿ ×¡×‘×™×¨
- ×œ× ×¨×¦×™× ×• ×œ×¡×›×Ÿ ××ª ×”×“×™×•×§ ×©×œ 97%
- Quantization ×“×•×¨×© ×‘×“×™×§×•×ª × ×•×¡×¤×•×ª

---

## ğŸ“ˆ ×”×©×•×•××” ××¡×›××ª

### ğŸ† ×˜×‘×œ×ª ×‘×™×¦×•×¢×™× ××œ××”

| ××“×“ | DeBERTa v3 | RoBERTa | MODELV2 One-Pass |
|-----|------------|---------|------------------|
| **Macro F1** | **97.07%** | 95.46% | 48.30% |
| **Accuracy** | 97.62% | ~95% | 96.98% |
| **Precision (No Match)** | 99.02% | ~98% | 97.03% |
| **Recall (No Match)** | 98.02% | ~97% | 99.96% |
| **Precision (Implicit)** | 89.97% | ~88% | 84.85% |
| **Recall (Implicit)** | **94.77%** | ~92% | 10.88% âŒ |
| **Precision (Explicit)** | 99.51% | ~98% | 89.62% |
| **Recall (Explicit)** | **99.67%** | ~98% | 15.32% âŒ |

### ğŸ“Š ××” ×”××¡×¤×¨×™× ××•××¨×™×?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ×”×¡×‘×¨ ×¢×œ ×”××“×“×™×                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  Precision = ×›××” ××”×—×™×–×•×™×™× ×©×œ× ×• × ×›×•× ×™×?                            â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                               â”‚
â”‚  ××ª×•×š ×›×œ ×”×¤×¢××™× ×©×”××•×“×œ ×××¨ "×™×© Python" - ×›××” ×¤×¢××™× ×¦×“×§?           â”‚
â”‚                                                                     â”‚
â”‚  DeBERTa: 99% â†’ ×›××¢×˜ ×ª××™×“ ×¦×•×“×§ ×›×©×”×•× ××–×”×” ××™×•×× ×•×ª!                 â”‚
â”‚  MODELV2: 85% â†’ 15% False Positives                                â”‚
â”‚                                                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                     â”‚
â”‚  Recall = ×›××” ××”××™×•×× ×•×™×•×ª ×”×××™×ª×™×•×ª ××¦×× ×•?                          â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                            â”‚
â”‚  ××ª×•×š ×›×œ ×”×¤×¢××™× ×©×‘×××ª ×™×© Python - ×›××” ×–×™×”×™× ×•?                      â”‚
â”‚                                                                     â”‚
â”‚  DeBERTa: 95-99% â†’ ××•×¦× ×›××¢×˜ ×”×›×œ!                                  â”‚
â”‚  MODELV2: 10-15% â†’ ××¤×¡×¤×¡ 85-90% ××”××™×•×× ×•×™×•×ª!                       â”‚
â”‚                                                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                     â”‚
â”‚  Macro F1 = ×××•×¦×¢ ×”×¨××•× ×™ ×©×œ Precision ×•-Recall                     â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                        â”‚
â”‚  × ×•×ª×Ÿ ××©×§×œ ×©×•×•×” ×œ×›×œ class, ×’× ×× ×”× ×œ× ×××•×–× ×™×                     â”‚
â”‚                                                                     â”‚
â”‚  DeBERTa: 97% â†’ ×‘×™×¦×•×¢×™× ××¢×•×œ×™× ×‘×›×œ ×”×§×˜×’×•×¨×™×•×ª                       â”‚
â”‚  MODELV2: 48% â†’ Recall × ××•×š ×”×•×¨×¡ ××ª ×”×¦×™×•×Ÿ                          â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### âš–ï¸ ××ª×™ ×œ×”×©×ª××© ×‘×›×œ ××•×“×œ?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ×”××œ×¦×•×ª ×©×™××•×©                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  âœ… DeBERTa v3 Base (××•××œ×¥!):                                      â”‚
â”‚     â€¢ ×›×©×¦×¨×™×š ×“×™×•×§ ××§×¡×™××œ×™                                          â”‚
â”‚     â€¢ ×›×©×—×©×•×‘ ×œ× ×œ×¤×¡×¤×¡ ××™×•×× ×•×™×•×ª (high recall)                      â”‚
â”‚     â€¢ ×›×©×™×© GPU ×–××™×Ÿ                                                â”‚
â”‚     â€¢ ×œ×™×™×¦×•×¨ (production)                                          â”‚
â”‚                                                                     â”‚
â”‚  âš ï¸ RoBERTa Base (×—×œ×•×¤×” ×˜×•×‘×”):                                     â”‚
â”‚     â€¢ ×›×©×¦×¨×™×š ×œ×—×¡×•×š ×–×™×›×¨×•×Ÿ (~230MB ×¤×—×•×ª)                            â”‚
â”‚     â€¢ ×›×©××¡×¤×™×§ ×“×™×•×§ ×©×œ 95%                                          â”‚
â”‚     â€¢ ×›××œ×˜×¨× ×˜×™×‘×” ×× DeBERTa ×œ× × ×˜×¢×Ÿ                                â”‚
â”‚     â€¢ ×œ×¤×™×ª×•×— ×•×‘×“×™×§×•×ª                                               â”‚
â”‚                                                                     â”‚
â”‚  âŒ MODELV2 One-Pass (×œ× ×œ×©×™××•×©!):                                 â”‚
â”‚     â€¢ ×¨×§ ×œ× ×™×¡×•×™×™× ×•××—×§×¨                                            â”‚
â”‚     â€¢ ××¤×¡×¤×¡ 85% ××”××™×•×× ×•×™×•×ª                                        â”‚
â”‚     â€¢ ××œ ×ª×©×ª××©×• ×‘×• ×‘×™×™×¦×•×¨                                          â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸš€ ×”××œ×¦×” ×¡×•×¤×™×ª

**×”×©×ª××©×• ×‘-DeBERTa v3 Base** ğŸ¯

| ×¡×™×‘×” | ×¤×™×¨×•×˜ |
|------|-------|
| ×‘×™×¦×•×¢×™× | F1 ×©×œ 97% - ×”×’×‘×•×” ××›×•×œ× |
| Recall | ××–×”×” 95-99% ××”××™×•×× ×•×™×•×ª |
| ×™×¦×™×‘×•×ª | ××™××•×Ÿ ×™×¦×™×‘, ×ª×•×¦××•×ª ×¢×§×‘×™×•×ª |
| ×”×‘× ×” | Disentangled Attention ××‘×™×Ÿ ×”×§×©×¨×™× ××•×¨×›×‘×™× |
| Vocabulary | 128K tokens ××›×¡×” ××•× ×—×™× ×˜×›× ×™×™× |

**228MB ×”× ×•×¡×¤×™× ×©×•×•×™× ××ª ×”×“×™×•×§!**

---

## ğŸ”§ ×¤×§×•×“×•×ª ×©×™××•×©×™×•×ª

### ×”×¨×¦×ª × ×™×ª×•×— ×¢× DeBERTa
```powershell
python analyze_resume.py --text "Your resume text here" --model deberta
```

### ×”×¨×¦×ª × ×™×ª×•×— ×¢× RoBERTa
```powershell
python analyze_resume.py --text "Your resume text here" --model roberta
```

### ×”×©×•×•××ª ×›×œ ×”××•×“×œ×™×
```powershell
python analyze_resume.py --text "Your resume text here" --model all
```

### × ×™×ª×•×— ×§×•×‘×¥ ×¢× ××¡×¤×¨ ×˜×§×¡×˜×™×
```powershell
python batch_analyze.py --input data.jsonl --output results.jsonl --model deberta
```

---

## ğŸ“š ××§×•×¨×•×ª ×•××××¨×™×

1. **DeBERTa Paper**: [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) - He et al., 2020
2. **RoBERTa Paper**: [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) - Liu et al., 2019
3. **BERT Paper**: [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805) - Devlin et al., 2018
4. **Attention Paper**: [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Vaswani et al., 2017
5. **HuggingFace Transformers**: [Documentation](https://huggingface.co/docs/transformers)

---

## ğŸ“ × ×¡×¤×—: ××™×œ×•×Ÿ ××•× ×—×™×

| ××•× ×— | ×”×¡×‘×¨ |
|------|------|
| **Transformer** | ××¨×›×™×˜×§×˜×•×¨×ª ×¨×©×ª ×¢×¦×‘×™×ª ××‘×•×¡×¡×ª Attention |
| **Attention** | ×× ×’× ×•×Ÿ ×©×××¤×©×¨ ×œ××•×“×œ "×œ×”×ª××§×“" ×‘×—×œ×§×™× ×¨×œ×•×•× ×˜×™×™× |
| **Tokenization** | ×¤×™×¨×•×§ ×˜×§×¡×˜ ×œ×˜×•×§× ×™× (××™×œ×™×/×—×œ×§×™ ××™×œ×™×) |
| **Embedding** | ×™×™×¦×•×’ ××™×œ×” ×›×•×•×§×˜×•×¨ ××¡×¤×¨×™ |
| **Fine-tuning** | ××™××•×Ÿ ××•×“×œ ×§×™×™× ×¢×œ × ×ª×•× ×™× ×¡×¤×¦×™×¤×™×™× |
| **F1 Score** | ×××•×¦×¢ ×”×¨××•× ×™ ×©×œ Precision ×•-Recall |
| **Macro F1** | F1 ×××•×¦×¢ ×¢×œ ×›×œ ×”×§×˜×’×•×¨×™×•×ª (××©×§×œ ×©×•×•×”) |
| **Pairwise** | ×’×™×©×” ×©×‘×•×“×§×ª ×–×•×’×•×ª (skill, text) |
| **One-Pass** | ×’×™×©×” ×©×× ×‘××ª ×”×›×œ ×‘×”×¨×¦×” ××—×ª |
| **CLS Token** | ×˜×•×§×Ÿ ××™×•×—×“ ×©××™×™×¦×’ ××ª ×›×œ ×”×§×œ×˜ |
| **Hidden Size** | ××™××“ ×”×™×™×¦×•×’ ×”×¤× ×™××™ (768 ××¦×œ× ×•) |
| **Parameters** | ×”××©×§×œ×•×ª ×”× ×œ××“×•×ª ×©×œ ×”×¨×©×ª |

---

*××¡××š × ×•×¦×¨ â€¢ ×™× ×•××¨ 2026 â€¢ SkillSight Project*
*×’×¨×¡×” 2.0 - ×¢× ×”×¡×‘×¨×™× ××¢××™×§×™× ×¢×œ ×”××œ×’×•×¨×™×ª××™× ×•×”×™×™×©×•×*
